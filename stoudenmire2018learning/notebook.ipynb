{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "- `Main Author:` E. Miles Stoudenmire\n",
    "- `Source:` [ArXiv](https://arxiv.org/abs/1801.00315)\n",
    "- `Publish Date:` 31-12-2017\n",
    "- `Reviewed Date:` 07-06-2021\n",
    "\n",
    "## Citation\n",
    "\n",
    "```latex\n",
    "@article{stoudenmire2018learning,\n",
    "  title={Learning relevant features of data with multi-scale tensor networks},\n",
    "  author={Stoudenmire, E Miles},\n",
    "  journal={Quantum Science and Technology},\n",
    "  volume={3},\n",
    "  number={3},\n",
    "  pages={034003},\n",
    "  year={2018},\n",
    "  publisher={IOP Publishing}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Iterable, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitwise operations to determine powers of two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is needed as the paper assumes the tree contains a number of nodes that is some power of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_power_of_two(n: int) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a given number is a power of two.\n",
    "    \"\"\"\n",
    "    return (n & (n-1) == 0) and n != 0\n",
    "\n",
    "def power_of_two(n: int) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Given a number, returns it's previous \n",
    "    and next of keen which are powers of two.\n",
    "    \"\"\"\n",
    "    if is_power_of_two(n):\n",
    "        return n, n\n",
    "    count = 0    \n",
    "    while n != 0:\n",
    "        n >>= 1\n",
    "        count += 1\n",
    "    prec = 1 << (count-1)\n",
    "    succ = 1 << count\n",
    "    return prec, succ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quickly checking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, (128, 256))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_power_of_two(13), is_power_of_two(256), power_of_two(196)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sliding window iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(xs: Iterable,\n",
    "                   size: int, \n",
    "                   step: Optional[int]=1,\n",
    "                   complement: Optional[bool]=False) -> Iterable:\n",
    "    \"\"\"\n",
    "    Sliding window iterator\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    xs:             Iterable.\n",
    "    size:           Window size.\n",
    "    step:           Step size.\n",
    "    complement:     Whether to return a complement.\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    Iterable\n",
    "    \"\"\"\n",
    "    assert step > 0, \"Parameter 'step' must be > 0\"\n",
    "    N = len(xs)\n",
    "    i = 0\n",
    "    while i + size <= N:\n",
    "        mask = np.ones(N, dtype=bool)\n",
    "        mask[i:i+size] = False\n",
    "        if complement:\n",
    "             yield xs[~mask], xs[mask]\n",
    "        else:\n",
    "            yield xs[~mask]\n",
    "        i += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quickly checking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [2 3] [4 5] [6 7] [8 9]\n"
     ]
    }
   ],
   "source": [
    "window_iterator = sliding_window(np.arange(10), size=2, step=2)\n",
    "print(*window_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some linear algebra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ishermitian(a: np.ndarray, rtol=1e-06, atol=1e-08) -> bool:\n",
    "    \"\"\"Check if the matrix is Hermitian\"\"\"\n",
    "    return np.allclose(a, a.conj().T, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ispsd(a: np.ndarray) -> bool:\n",
    "    \"\"\"Check if a matrix is positive definite.\"\"\"\n",
    "    return np.all(np.linalg.eigvals(a) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(data_path: str) -> np.ndarray:\n",
    "    num_labels = 10\n",
    "    img_size = 28\n",
    "\n",
    "    # data files\n",
    "    train_data_path = os.path.join(data_path, 'mnist_train.csv')\n",
    "    test_data_path = os.path.join(data_path, 'mnist_test.csv')\n",
    "\n",
    "    # loading data\n",
    "    train_data = np.loadtxt(train_data_path, delimiter=',')\n",
    "    test_data = np.loadtxt(test_data_path, delimiter=',')\n",
    "\n",
    "    # TLDR; scaling to [0.01, 0.99]; we want to avoid zeros and ones\n",
    "    # Longer answer: https://arxiv.org/pdf/1512.00567.pdf\n",
    "    frac = 0.99 / 255\n",
    "\n",
    "    # extracting data and labels\n",
    "    train_images = np.asfarray(train_data[:, 1:]) * frac + 0.01\n",
    "    train_labels = np.asfarray(train_data[:, :1])\n",
    "\n",
    "    test_images = np.asfarray(test_data[:, 1:]) * frac + 0.01\n",
    "    test_labels = np.asfarray(test_data[:, :1])\n",
    "\n",
    "    # transform labels into one hot representation\n",
    "    lr = np.arange(num_labels)\n",
    "    train_labels_one_hot = (lr==train_labels).astype(float)\n",
    "    test_labels_one_hot = (lr==test_labels).astype(float)\n",
    "\n",
    "    # we don't want zeroes and ones in the labels either\n",
    "    train_labels_one_hot[train_labels_one_hot==0] = 0.01\n",
    "    train_labels_one_hot[train_labels_one_hot==1] = 0.99\n",
    "    test_labels_one_hot[test_labels_one_hot==0] = 0.01\n",
    "    test_labels_one_hot[test_labels_one_hot==1] = 0.99\n",
    "\n",
    "    return {\n",
    "        'train': {\n",
    "            'images': train_images,\n",
    "            'labels': train_labels_one_hot\n",
    "        },\n",
    "        'test': {\n",
    "            'images': test_images,\n",
    "            'labels': test_labels_one_hot\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_mnist('../data/mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly checking that data is loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 10   # a particularr image index\n",
    "img_vec = data['train']['images'][j]\n",
    "size = np.sqrt(img_vec.size).astype(int)\n",
    "img = img_vec.reshape(size, size)\n",
    "\n",
    "plt.imshow(img, cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to rescale image so the size of eacch image is a power of $2$. This is required for the algorithm to work. First, we determine the nearest powers of $2$ given the size of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 10   # a particularr image index\n",
    "img_vec = data['train']['images'][j]\n",
    "size_ = np.sqrt(img_vec.size).astype(int)\n",
    "size, _ = power_of_two(size_)   # returns nearest powers of 2\n",
    "\n",
    "print(f'The suggested new size is: {size}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we downscale each image to this new size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image_vec: np.ndarray, shape: Tuple[int,int]) -> np.ndarray:\n",
    "    orig_size = np.sqrt(img_vec.size).astype(int)\n",
    "    image = image_vec.reshape(orig_size, orig_size)\n",
    "    image = cv2.resize(image, shape, interpolation=cv2.INTER_CUBIC)\n",
    "    return image.reshape(-1,1)\n",
    "    \n",
    "new_shape = (8,8)  # I actually downscale even more to save exec time\n",
    "\n",
    "train_images = np.squeeze(np.apply_along_axis(lambda x: resize(x, new_shape), axis=1, arr=data['train']['images']))\n",
    "test_images = np.squeeze(np.apply_along_axis(lambda x: resize(x, new_shape), axis=1, arr=data['test']['images']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if data is scaled properly by plotting random digits..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_digit(image_vec: np.ndarray) -> None:\n",
    "    assert image_vec.ndim == 1\n",
    "    size = np.sqrt(image_vec.size).astype(int)\n",
    "    plt.rcParams[\"figure.figsize\"] = (2,2)\n",
    "    plt.imshow(image_vec.reshape(size, size), cmap=\"Greys\")\n",
    "    plt.show()\n",
    "\n",
    "digits_idx = (0,23,157,996)    # plot random images\n",
    "for j in digits_idx:\n",
    "    show_digit(train_images[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature map $\\Phi(\\bf x)$ maps inpunt data vector $\\bf x$ from a space of dimension $N$ to a space of dimension $d^N$, i.e. each component of the input vector $x_j$ is mapped into a $d$-dimensional vector. See Eq. 8 in text.\n",
    "\n",
    "The tensor $\\Phi^{s_1 s_2 \\cdots s_N}$ is the tensor productof the local feature maps $\\Phi^{s_j}(x_j)$ applied to each input $x_j$ along the indices $s_j = 1,2,\\cdots,N$. The local feature map is required to have unit norm analogous to the wave function norm, otherwise the tensor networks are not guaranteed to be numerically stable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FMap = Callable[np.ndarray, np.ndarray] # type alias\n",
    "\n",
    "def feature_map(xs: Iterable, \n",
    "                f: FMap,\n",
    "                *fs: Iterable[FMap],\n",
    "                acc: Optional[np.ndarray]=None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Feature mapping for a single data sample (image vector).\n",
    "    Input:\n",
    "    ------\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "    An array containing feature values for each vector value.\n",
    "    The size of the output is features x vector\n",
    "    \"\"\"\n",
    "    vf = np.vectorize(f)\n",
    "    acc = vf(xs) if acc is None else np.r_[acc, vf(xs)]\n",
    "    if fs:\n",
    "        yield from feature_map(xs, *fs, acc=acc)\n",
    "    else:\n",
    "        yield acc\n",
    "\n",
    "def tensorize_dataset(a: np.ndarray, *fs: Iterable[FMap]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The tensorization of the dataset.\n",
    "    Input:\n",
    "    ------\n",
    "    a:      Dataset containing images with pixels aranged as a long vector.\n",
    "    fs:     Features\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "    \"\"\"\n",
    "    n,m = a.shape\n",
    "    # feature extractor iterator\n",
    "    feat_extractor_iterator = feature_map(a,*fs)\n",
    "    # creating features\n",
    "    phi = np.vstack(*feat_extractor_iterator)\n",
    "    # rearranging features for convenience\n",
    "    return phi.reshape(len(fs),n,m).transpose(0,2,1)\n",
    "\n",
    "features = [\n",
    "    lambda _: 1,\n",
    "    lambda x: x,\n",
    "]\n",
    "# features = [\n",
    "#   lambda x: np.cos( (np.pi/2) * x ),\n",
    "#   lambda x: np.sin( (np.pi/2) * x ),\n",
    "# ]\n",
    "phi_train = tensorize_dataset(train_images, *features)\n",
    "phi_test = tensorize_dataset(test_images, *features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the features so we can trivially contract them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_along_axis(a: np.ndarray, \n",
    "#                          ord: Optional[int] = 2,\n",
    "#                          axis: Optional[int] = -1) -> np.ndarray:\n",
    "#     z = np.atleast_1d(np.linalg.norm(a, ord, axis))\n",
    "#     z[z==0] = 1\n",
    "#     return a / np.expand_dims(z, axis)\n",
    "#\n",
    "#Phi = normalize_along_axis(phi_train, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking the normalization on a random sample of features and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_size = 10\n",
    "#\n",
    "# for i in range(sample_size):\n",
    "#     m = np.random.choice(phi_train.shape[1])\n",
    "#     n = np.random.choice(phi_train.shape[2])\n",
    "#     assert np.allclose(phi_train[:,m,n].T @ phi_train[:,m,n], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if images are correctly processed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 157                       # specific image id\n",
    "img = phi_train[:,:,j]        # each image is a (1,x) represenattion\n",
    "show_digit(train_images[j])   # original image\n",
    "show_digit(img[1,:])          # image asfter the feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the reduced covariance matrix and truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_covariance(a: np.ndarray,\n",
    "                        eps: Optional[float]=1e-3) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    \"\"\"\n",
    "    Approximate the input covariance matrix by a truncated version.\n",
    "    Uses the eigendecomposition to diagonaize the input matrix\n",
    "    and truncates the spectrum p that satisfies the condition\n",
    "    sum(p)/trace(a) < eps.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    a:      Matrix to be truncated.\n",
    "    eps:    The truncation error.\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "    p:      The remaining spectrum.\n",
    "    U:      The eigenvectors corresponding to p.\n",
    "    D:      The index of the smallest non-negligible eigenalue.\n",
    "    \"\"\"\n",
    "    assert ishermitian(a), 'Input matrix is not Hermitian.'    \n",
    "    # the eignevalue decomposition\n",
    "    p, U = np.linalg.eigh(a)  # p are sorted in ascending order\n",
    "    # checking the contribution of eignevalues and get the index D\n",
    "    err = np.cumsum(p / np.trace(a))  # errors see Eq. (19) in text\n",
    "    idx = np.amin(np.where(err >= np.complex64(eps))[0])  # non-negligible errros\n",
    "    return p, U, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quickly checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(111)\n",
    "v = np.random.rand(30,10) + 1j*np.random.rand(30,10)\n",
    "a = v @ v.conj().T  # covariance matrix\n",
    "\n",
    "p,U,D = truncate_covariance(a)\n",
    "assert np.allclose((U[:,D:] * p[D:]) @ U[:,D:].conj().T, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go(a: np.ndarray, eps: Optional[float]=1e-3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    A single step building a tree layer.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    a:      Array of input data used to construct the isometries.\n",
    "    eps:    Truncation tolerance.\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "    A tensor representing the isometries.\n",
    "    \"\"\"\n",
    "    # getting the shape of the input tensor\n",
    "    u,v,w = a.shape\n",
    "    # precomputes all density matrices\n",
    "    rho = np.einsum('ilj,klj->iklj', a, a.conj())\n",
    "    # initialize the isometries\n",
    "    isometries  = np.empty(shape=(u**2, u**2, 0), dtype=np.double)\n",
    "    # initialize the trancation idices for each isometry\n",
    "    indices = np.empty(shape=(v//2, 0), dtype=int)\n",
    "\n",
    "    # gets the sliding window iterator\n",
    "    iterator = sliding_window(np.arange(v), step=2, size=2)\n",
    "    \n",
    "    for i,j in iterator:\n",
    "\n",
    "        if i == 0:\n",
    "            # an edge case\n",
    "            left = np.ones(w)\n",
    "        else:\n",
    "            # contraction of all parts to the left of the pointer (i.e. l)\n",
    "            left = np.einsum('ikj,ikj->j', a[:,:i,:], a[:,:i,:].conj())\n",
    "\n",
    "        if j == v-1:\n",
    "            # an edge case\n",
    "            right = np.ones(w)\n",
    "        else:\n",
    "            # contraction all parts to the right of the pointer (i.e. r+1)\n",
    "            right = np.einsum('ikj,ikj->j', a[:,j+1:,:], a[:,j+1:,:].conj())\n",
    "\n",
    "        # reduced density matrix for the window\n",
    "        rho_ij = np.einsum('ikj,mnj->imknj', rho[:,:,i,:], rho[:,:,j,:])\n",
    "        # scaled by left and right parts (aka reduced density matrix)\n",
    "        rho_ij = np.einsum('...j,...j,...j', left, rho_ij, right)\n",
    "        \n",
    "        # eigen decomposition\n",
    "        _, U, idx = truncate_covariance(rho_ij.reshape(u**2, u**2), eps)\n",
    "\n",
    "        isometries = np.append(isometries, U[:,:,np.newaxis], axis=2)\n",
    "        indices = np.append(indices, idx)\n",
    "\n",
    "    # get a uniform trancation index for ALL isometries (see Eq. 19)\n",
    "    D = np.min(indices)\n",
    "    # trancating\n",
    "    isometries = isometries[:,D:,:].reshape(u, u, -1, v//2)                   \n",
    "    return isometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(a, iso: np.ndarray, *other: Iterable[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    A projector of input tesnor 'a' onto the isometries.\n",
    "    The tensor is first split into 2 parts corresponding\n",
    "    to two input legs of the isoemtry tensor (see picture\n",
    "    below for details).\n",
    "           out      \n",
    "            | \n",
    "           iso  \n",
    "           / \\ \n",
    "    a -> a1   a2\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    a:      Tesnor to be projected.\n",
    "    iso:    Tensor with isometries to be projected on.\n",
    "    other:  A tail of the iterable containing isometries. \n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "    A result of the projection.\n",
    "    \"\"\"\n",
    "    # split the array in two parts (correspond to two legs of the input)\n",
    "    a1, a2 = np.split(a, 2, axis=1)\n",
    "    # projection of a onto isometries\n",
    "    out = np.einsum('ilk,jlk,ijml->mlk', a1, a2, iso)\n",
    "    if other:\n",
    "        return project(out, *other)\n",
    "    else:\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(a: np.ndarray, eps: Optional[float]=1e-3) -> Iterable[np.ndarray]:\n",
    "    \"\"\"\n",
    "    A tree constructor.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    a:      Array with input data to the tree constructor.\n",
    "    eps:    Trancation tolerance.\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "    An iterator spitting the isometries for each tree layer.\n",
    "    \"\"\"\n",
    "    # getting the shape of the input tensor\n",
    "    u,v,w = a.shape\n",
    "\n",
    "    assert is_power_of_two(v), 'The number of features must be a power of 2'\n",
    "\n",
    "    # process until stopping criterium\n",
    "    if v > 2:\n",
    "        isometries = go(a, eps=eps)\n",
    "        yield isometries\n",
    "        # creating the input for the next step\n",
    "        a_new = project(a, isometries)\n",
    "        yield from build_tree(a_new, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_iterator = build_tree(phi_train, eps=7e-2)\n",
    "# reuse the expensive iterator\n",
    "tree_train, tree_test = itertools.tee(tree_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data after coarse graining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = data['train']['labels']\n",
    "test_labels = data['test']['labels']\n",
    "\n",
    "train_data = project(phi_train, *tree_train)\n",
    "test_data = project(phi_test, *tree_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I want to compare my implementation of CGD to Sklearn Logistic model. Start with Logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j,n = train_data.shape\n",
    "k,l,m = test_data.shape\n",
    "\n",
    "assert i==k and j==l, 'Data shapes mismatch'\n",
    "\n",
    "X_train = train_data.reshape(i*j,n).T\n",
    "X_test = test_data.reshape(k*l,m).T\n",
    "\n",
    "y_train = np.argwhere(train_labels==.99)[:,1]  # need to convert it back as Sklearn require raw labels\n",
    "y_test = np.argwhere(test_labels==.99)[:,1]    # same here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(multi_class='ovr', solver='newton-cg')  # the CGD classifier is also OvA\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(f\"Classification report for classifier {model}:\\n\\n\"\n",
    "      f\"{metrics.classification_report(y_test, y_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Classification confusion matrix:\\n\\n'\n",
    "      f'{metrics.confusion_matrix(y_test, y_pred)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot normalized confusion matrix\n",
    "# fig, ax = plt.subplots(figsize=(12, 12))\n",
    "# disp = metrics.plot_confusion_matrix(\n",
    "#     model, X_test, y_test, \n",
    "#     display_labels=np.arange(10),\n",
    "#     cmap=plt.cm.Blues, \n",
    "#     ax=ax,\n",
    "#     normalize='true')\n",
    "# disp.ax_.set_title(f'Accuracy (Tree TN Compression): {metrics.accuracy_score(y_test, y_pred)}')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on the original dataset for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I want to see how much 'information' is lost after the coarse graining. For this, I train the same logistic regression classifier as above on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_images\n",
    "X_test = test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_orig = LogisticRegression(multi_class='ovr', solver='newton-cg')  # same again, CGD is OvA and I want direct comparison\n",
    "model_orig.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_orig = model_orig.predict(X_test)\n",
    "print(f\"Classification report for classifier {model}:\\n\\n\"\n",
    "      f\"{metrics.classification_report(y_test, y_pred_orig)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Classification confusion matrix:\\n\\n'\n",
    "      f'{metrics.confusion_matrix(y_test, y_pred_orig)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My conjugate gradient method implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to follow the paper as close as possible (and also learn something along the line). So, I have decided to implement a custom conjugate gradient descent algorithm that the paper is using as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(A, b, tol=1e-6):\n",
    "    x = np.ones_like(b)\n",
    "    r = A @ x - b\n",
    "    d = -r\n",
    "    while True:\n",
    "        r2 = r @ r\n",
    "        if np.sqrt(r2) < tol:\n",
    "            break\n",
    "        Ad = A @ d\n",
    "        alpha = r2 / (d @ Ad)\n",
    "        x = x + alpha * d\n",
    "        r = r + alpha * Ad\n",
    "        beta = r @ r / r2\n",
    "        d = -r + beta * d\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it on a randomly generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "np.random.seed(0)\n",
    "A = np.random.normal(size=(n,n))\n",
    "A = A.T @ A\n",
    "\n",
    "# below are the requirements of the CGD algorithm.\n",
    "assert ispsd(A), 'The matrix is not PSD'\n",
    "assert ishermitian, 'The matrix is not Hermitian'    \n",
    "\n",
    "b = np.random.rand(n)\n",
    "%time x = conjugate_gradient(A, b)\n",
    "np.allclose(A @ x, b, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a classifier for each digit separately using CGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j,n = train_data.shape\n",
    "k,l,m = test_data.shape\n",
    "\n",
    "assert i==k and j==l, 'Data shapes mismatch'\n",
    "\n",
    "X_train = train_data.reshape(i*j,n).T\n",
    "X_test = test_data.reshape(k*l,m).T\n",
    "\n",
    "y_train = np.argwhere(train_labels==.99)[:,1]\n",
    "y_test = np.argwhere(test_labels==.99)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_build(X, y):\n",
    "    \"\"\"\n",
    "    Train a OvA classifier.\n",
    "    \"\"\"\n",
    "    # getting the shape\n",
    "    m,n = X.shape\n",
    "\n",
    "    # assumed number of digits in the dataset\n",
    "    n_digits = 10\n",
    "    \n",
    "    # storing weights\n",
    "    W = np.zeros(shape=(n_digits,n+1), dtype=np.double)\n",
    "    \n",
    "    # adding bias term\n",
    "    X_ = np.ones(shape=(m,n+1), dtype=np.double)\n",
    "    X_[:,1:] = X\n",
    "\n",
    "    # optimize the weights for each digit\n",
    "    for digit in np.arange(n_digits):\n",
    "        y_ = np.where(y != digit, 0.01, 0.99)\n",
    "        # train cgd with square loss\n",
    "        Z = X_.T @ X_\n",
    "        q = X_.T @ y_\n",
    "        W[digit,:] = conjugate_gradient(Z,q)\n",
    "    # returns a set of parameters n_classes x n_features\n",
    "    return W\n",
    "\n",
    "def clf_predict(W, X):\n",
    "    # resizing the arrray if one sample provided\n",
    "    if X.ndim == 1:\n",
    "        X = X[np.newaxis,:]\n",
    "    # getting the shape\n",
    "    m,n = X.shape\n",
    "    # assumed number of digits in the dataset\n",
    "    n_digits = 10\n",
    "    # adding bias term\n",
    "    X_ = np.ones(shape=(m,n+1), dtype=np.double)\n",
    "    X_[:,1:] = X\n",
    "    # stores predictions\n",
    "    y_pred = np.zeros(shape=(m, n_digits), dtype=np.double)\n",
    "    for digit in np.arange(n_digits):\n",
    "        y_pred[:,digit] = X_ @ W[digit,:]\n",
    "    return np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = clf_build(X_train, y_train)\n",
    "y_pred = clf_predict(W, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now also check a single sample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Classification report for classifier {model}:\\n\\n'\n",
    "      f'{metrics.classification_report(y_test, y_pred)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Classification confusion matrix:\\n\\n'\n",
    "      f'{metrics.confusion_matrix(y_test, y_pred)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute force method (easy to understand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "phi_rand = np.random.rand(3, 196, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "def exec_time(tic, toc):\n",
    "   diff_time = toc - tic\n",
    "   ms = diff_time\n",
    "   m, s = divmod(diff_time, 60)\n",
    "   h, m = divmod(m, 60)\n",
    "   s,m,h = int(round(s, 0)), int(round(m, 0)), int(round(h, 0))\n",
    "   print('Execution Time: ' + f'{h:02d}:{m:02d}:{s:02d}:{ms:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l,r = 8,9\n",
    "u,v,w = phi_rand.shape\n",
    "\n",
    "tic = timer()\n",
    "\n",
    "rho1 = np.zeros([u,u,u,u])\n",
    "for j in range(w):\n",
    "    left = np.einsum('ik,ik', phi_rand[:,:l,j], phi_rand[:,:l,j].conj())\n",
    "    right = np.einsum('ik,ik', phi_rand[:,r+1:,j], phi_rand[:,r+1:,j].conj())\n",
    "\n",
    "    # i   m\n",
    "    # |   |\n",
    "    # +---+\n",
    "    # |   |\n",
    "    # +---+\n",
    "    # |   |\n",
    "    # k   n\n",
    "\n",
    "    for i in range(u):\n",
    "        for k in range(u):\n",
    "            for m in range(u):\n",
    "                for n in range(u):\n",
    "                    rho1[i,k,m,n] += left * right * phi_rand[i,l,j] * phi_rand[m,l,j].conj() * phi_rand[k,r,j] * phi_rand[n,r,j].conj()\n",
    "\n",
    "toc = timer()\n",
    "time1 = toc-tic\n",
    "\n",
    "exec_time(tic,toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same as above, but using vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "\n",
    "u,v,w = phi_rand.shape\n",
    "\n",
    "l,r = 8,9\n",
    "\n",
    "# computes all density matrices for each j\n",
    "rho2 = np.einsum('ilj,klj->iklj', phi_rand, phi_rand.conj())\n",
    "# contraction of all parts to the left of the pointer (i.e. l)\n",
    "left = np.einsum('ikj,ikj->j', phi_rand[:,:l,:], phi_rand[:,:l,:].conj())\n",
    "# contraction all parts to the right of the pointer (i.e. r+1)\n",
    "right = np.einsum('ikj,ikj->j', phi_rand[:,r+1:,:], phi_rand[:,r+1:,:].conj())\n",
    "# reduced density matrix for the window\n",
    "rho2 = np.einsum('ikj,mnj->imknj', rho2[:,:,l,:], rho2[:,:,r,:])\n",
    "# scaled by left and right\n",
    "rho2 = np.einsum('...j,...j,...j', rho2, left, right)\n",
    "\n",
    "toc = timer()\n",
    "time2 = toc-tic\n",
    "\n",
    "exec_time(tic,toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking that the results are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(rho1, rho2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 / time2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2262ef1e1764844cc7dddacc5332fc08dc39a3661d4c909d0d650c194bf9ca7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
